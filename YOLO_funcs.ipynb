{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOw0LvtI6T8hA3wUnwMhmCT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dovkess/FGCVBreedDetection/blob/main/YOLO_funcs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hiVhv-1yDI1"
      },
      "outputs": [],
      "source": [
        "# Import cell. this is a cell run once to avoid having import problems later on.\n",
        "!pip install tensorflow\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications import VGG16, InceptionResNetV2, InceptionV3, NASNetLarge#, PNASNetLarge\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, GlobalAveragePooling2D\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import time\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "%pip install lime\n",
        "import lime\n",
        "from lime import lime_image\n",
        "from skimage.segmentation import mark_boundaries\n",
        "\n",
        "!pip install ultralytics\n",
        "from ultralytics import YOLO\n",
        "import io\n",
        "import base64\n",
        "\n",
        "# Mount Google Drive -- if images are stored in drive. if not -- it's not needed.\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(model_version=8, model_size='x'):\n",
        "    '''\n",
        "    Get Yolo model\n",
        "    devault is v8x\n",
        "\n",
        "    '''\n",
        "    model = YOLO('yolov{}{}.pt'.format(model_version, model_size))\n",
        "    return model"
      ],
      "metadata": {
        "id": "adH3tuNyyLdv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I7_C0rXlc6am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_dogs(model, image_path, dir=False, g=False, batch_size=1):\n",
        "    '''\n",
        "    Count dogs in the image.\n",
        "\n",
        "    Args:\n",
        "        model: The YOLO model to use for detection.\n",
        "        image_path (str): The path to the image file.\n",
        "        dir (bool): is the image path a dir or a single image\n",
        "        g (bool): is it a dir of dirs with images\n",
        "        batch size (int): batch size to send to YOLO\n",
        "\n",
        "    Returns:\n",
        "        list (int): The number of dogs detected in the image.\n",
        "        list (int): number of cats detected in the image.\n",
        "        list (int): The number of animals detected in the image.\n",
        "        list (str): The paths to the image file.\n",
        "    '''\n",
        "    if not dir:\n",
        "        # Read the image\n",
        "        img = Image.open(image_path).convert(\"RGB\")\n",
        "    else:\n",
        "        img = image_path\n",
        "\n",
        "    if g:\n",
        "        img = '{}/*'.format(image_path)\n",
        "    animal_index = list(range(17, 24))\n",
        "    # Perform detection\n",
        "    results = model(img, device='cuda', batch=batch_size)\n",
        "    dogs = [sum(results[i].boxes.cls == 16.) for i in range(len(results))]\n",
        "    cats = [sum(results[i].boxes.cls == 15.) for i in range(len(results))]\n",
        "    animals = [sum([results[k].boxes.cls[i] in animal_index for i in range(len(results[k].boxes.cls))]) for k in range(len(results))]\n",
        "    p = [results[i].path for i in range(len(results))]\n",
        "    return dogs, cats, animals, p"
      ],
      "metadata": {
        "id": "2RDaWZWWCVgP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_dogs_and_draw_boxes(model, image_path):\n",
        "    \"\"\"\n",
        "    Reads an image, performs dog detection using YOLOv8, and draws bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        model: The YOLO model to use for detection.\n",
        "        image_path (str): The path to the image file.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor or None: The bounding box (x1, y1, x2, y2) of the detected dog\n",
        "                              with the highest confidence. Returns None if no dogs are detected.\n",
        "    \"\"\"\n",
        "    # Read the image\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Perform detection\n",
        "    results = model(img)\n",
        "    dog_indeces = results[0].boxes.cls == 16.\n",
        "    results = model(img, device='cuda', batch=batch_size)\n",
        "    dogs = [sum(results[i].boxes.cls == 16.) for i in range(len(results))]\n",
        "    cats = [sum(results[i].boxes.cls == 15.) for i in range(len(results))]\n",
        "    animals = [sum([results[k].boxes.cls[i] in animal_index for i in range(len(results[k].boxes.cls))]) for k in range(len(results))]\n",
        "    p = [results[i].path for i in range(len(results))]\n",
        "    if not dog_indeces.any():\n",
        "        return None\n",
        "    dog_confs = results[0].boxes.conf * dog_indeces\n",
        "    box = results[0].boxes.xyxy[dog_confs.argmax()]\n",
        "    return box"
      ],
      "metadata": {
        "id": "J9hsQMcV1CYf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bounding_box_dogs_and_cats(model, image_path, only_animals=True, good_class=[15, 16]):\n",
        "    \"\"\"\n",
        "    Reads an image, returns largest bounding box of \"good\" animals.\n",
        "    Since YOLO is not always rigth about classification, but does have good segmentation,\n",
        "    if we did not find \"good\" animals, we will take all.\n",
        "\n",
        "    Args:\n",
        "        model: The YOLO model to use for detection.\n",
        "        image_path (str): The path to the image file.\n",
        "        only_animals (bool): Only show animals (classes 15-23)\n",
        "        good_class (list): List of classes to consider as good\n",
        "\n",
        "    Returns:\n",
        "        bounding box of all objects detected in the image.\n",
        "        number of good objects detected in the image.\n",
        "        number of animals detected in the image.\n",
        "    \"\"\"\n",
        "    # Read the image\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    bounding_box = [math.inf, math.inf, -math.inf, -math.inf]\n",
        "    # Perform detection\n",
        "    results = model(img)\n",
        "    good_counter = 0\n",
        "    animals_counter = 0\n",
        "    for i in range(len(results[0].boxes.cls)):\n",
        "        if results[0].boxes.cls[i] in good_class:\n",
        "            good_counter += 1\n",
        "            bounding_box = [min(bounding_box[0], results[0].boxes.xyxy[i][0]), min(bounding_box[1], results[0].boxes.xyxy[i][1]),\n",
        "                            max(bounding_box[2], results[0].boxes.xyxy[i][2]), max(bounding_box[3], results[0].boxes.xyxy[i][3])]\n",
        "    if not good_counter:\n",
        "        for i in range(len(results[0].boxes.cls)):\n",
        "            if results[0].boxes.cls[i] in list(range(14, 25)):\n",
        "                animals_counter += 1\n",
        "                bounding_box = [min(bounding_box[0], results[0].boxes.xyxy[i][0]), min(bounding_box[1], results[0].boxes.xyxy[i][1]),\n",
        "                                max(bounding_box[2], results[0].boxes.xyxy[i][2]), max(bounding_box[3], results[0].boxes.xyxy[i][3])]\n",
        "    if not animals_counter and not good_counter:\n",
        "        for i in range(len(results[0].boxes.cls)):\n",
        "            bounding_box = [min(bounding_box[0], results[0].boxes.xyxy[i][0]), min(bounding_box[1], results[0].boxes.xyxy[i][1]),\n",
        "                            max(bounding_box[2], results[0].boxes.xyxy[i][2]), max(bounding_box[3], results[0].boxes.xyxy[i][3])]\n",
        "    return bounding_box if animals_counter < 7 else None, good_counter, animals_counter\n"
      ],
      "metadata": {
        "id": "Sv4rix83NP6E"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_classes(img, model):\n",
        "    \"\"\"\n",
        "    Reads an image, returns a list of classes in the image using YOLOv8\n",
        "\n",
        "    Args:\n",
        "        image_path (str): The path to the image file.\n",
        "        model: The YOLO model to use for detection.\n",
        "\n",
        "    Returns:\n",
        "        list of classes in the image.\n",
        "        list of names of classes in the image.\n",
        "    \"\"\"\n",
        "    results = model(img)\n",
        "    results_classes = results[0].boxes.cls.tolist()\n",
        "    return [results[0].names[results_classes[i]] for i in range(len(results_classes))], results[0].names\n"
      ],
      "metadata": {
        "id": "xrk6Ct5m2Tlk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_to_square_around_dog(original_img, box, dbg=False):\n",
        "    '''\n",
        "    Crop the image to a square around the dog.\n",
        "    If a square can't be created, we will pad the image (zero padding).\n",
        "    Args:\n",
        "        original_img (PIL.Image): The original image.\n",
        "        box (list): The bounding box of the dog.\n",
        "        dbg (bool): Whether to print debug information.\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image: The cropped and padded image.\n",
        "    '''\n",
        "    x1, y1, x2, y2 = box\n",
        "    if dbg:\n",
        "        print('boxes: {}'.format(box))\n",
        "    box_center_x = (x1 + x2) / 2\n",
        "    box_center_y = (y1 + y2) / 2\n",
        "    box_width = x2 - x1\n",
        "    box_height = y2 - y1\n",
        "\n",
        "    # Determine the side length of the square.\n",
        "    square_side = max(box_width, box_height)\n",
        "\n",
        "    # Calculate the initial coordinates for the square crop centered around the box center\n",
        "    crop_x1 = box_center_x - square_side / 2\n",
        "    crop_y1 = box_center_y - square_side / 2\n",
        "    crop_x2 = box_center_x + square_side / 2\n",
        "    crop_y2 = box_center_y + square_side / 2\n",
        "\n",
        "    img_width, img_height = original_img.size\n",
        "\n",
        "    # Calculate padding needed\n",
        "    pad_left = max(0, -crop_x1)\n",
        "    pad_top = max(0, -crop_y1)\n",
        "    pad_right = max(0, crop_x2 - img_width)\n",
        "    pad_bottom = max(0, crop_y2 - img_height)\n",
        "\n",
        "    # Adjust crop coordinates to be within original image bounds *before* padding\n",
        "    bounded_crop_x1 = max(0, crop_x1)\n",
        "    bounded_crop_y1 = max(0, crop_y1)\n",
        "    bounded_crop_x2 = min(img_width, crop_x2)\n",
        "    bounded_crop_y2 = min(img_height, crop_y2)\n",
        "\n",
        "    crop_box_bounded = (int(bounded_crop_x1), int(bounded_crop_y1), int(bounded_crop_x2), int(bounded_crop_y2))\n",
        "\n",
        "    try:\n",
        "        cropped_img = original_img.crop(crop_box_bounded)\n",
        "\n",
        "        # Pad the cropped image to make it a perfect square if necessary\n",
        "        padded_width = int(bounded_crop_x2 - bounded_crop_x1 + pad_left + pad_right)\n",
        "        padded_height = int(bounded_crop_y2 - bounded_crop_y1 + pad_top + pad_bottom)\n",
        "        final_square_side = max(padded_width, padded_height)\n",
        "        padded_img = Image.new('RGB', (final_square_side, final_square_side), (0, 0, 0))\n",
        "        paste_x = int(pad_left)\n",
        "        paste_y = int(pad_top)\n",
        "\n",
        "        padded_img.paste(cropped_img, (paste_x, paste_y))\n",
        "\n",
        "        return padded_img\n",
        "    except Exception as e:\n",
        "        print(f\"Error during cropping or padding: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "uxpfw7O42icD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_and_save(model, img_path, new_path, dbg=False, crop_func=detect_dogs_and_draw_boxes):\n",
        "    '''\n",
        "    Crop and save new images, using the above functions.\n",
        "    Args:\n",
        "        model (YOLO): The YOLO model to use for detection.\n",
        "        img_path (str): The path to the original image.\n",
        "        new_path (str): The path to save the new image.\n",
        "        dbg (bool): Whether to print debug information.\n",
        "        crop_func (function): The function to use for cropping.\n",
        "\n",
        "    Returns:\n",
        "        bool: Whether the cropping was successful.\n",
        "    '''\n",
        "    original_img = Image.open(img_path).convert(\"RGB\")\n",
        "    dog_results, good_count, animal_count = crop_func(model, img_path)\n",
        "    if dog_results is not None:\n",
        "        cropped_dog_image = crop_to_square_around_dog(original_img, dog_results, dbg=dbg)\n",
        "        if cropped_dog_image:\n",
        "            # for debuggning, we would like to display the cropped image.\n",
        "            if dbg:\n",
        "                display(original_img)\n",
        "                display(cropped_dog_image)\n",
        "                return True\n",
        "            cropped_dog_image.save(new_path)\n",
        "            return True\n",
        "        else:\n",
        "            print(\"Failed to crop and pad the image: {}\".format(img_path))\n",
        "            return False\n",
        "    else:\n",
        "        print(\"No valid dog bounding box result provided: {}\".format(img_path))\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "oz90DeOY3fQd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(img_path):\n",
        "    '''\n",
        "    Show image.\n",
        "    Args:\n",
        "        img_path (str): The path to the image.\n",
        "    '''\n",
        "    img_path = Image.open(img_path).convert(\"RGB\")\n",
        "    display(img_path)"
      ],
      "metadata": {
        "id": "3QsSnhRPDUis"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}